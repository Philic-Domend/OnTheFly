
üîç ÂÆåÊï¥Ê®°ÂûãÁªìÊûÑÔºö
============================================================
  1. model.action_in_proj.bias                          (1024,)              (1,024 params)
  2. model.action_in_proj.weight                        (1024, 32)           (32,768 params)
  3. model.action_out_proj.bias                         (32,)                (32 params)
  4. model.action_out_proj.weight                       (32, 1024)           (32,768 params)
  5. model.paligemma_with_expert.da_head.bias           (2048,)              (2,048 params)
  6. model.paligemma_with_expert.da_head.weight         (2048, 2048)         (4,194,304 params)
  7. model.paligemma_with_expert.discrete_action_embedding.weight (2048, 2048)         (4,194,304 params)
  8. model.paligemma_with_expert.gemma_expert.lm_head.weight (257152, 1024)       (263,323,648 params)
  9. model.paligemma_with_expert.gemma_expert.model.layers.0.input_layernorm.dense.bias (3072,)              (3,072 params)
 10. model.paligemma_with_expert.gemma_expert.model.layers.0.input_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
 11. model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.down_proj.weight (1024, 4096)         (4,194,304 params)
 12. model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.gate_proj.weight (4096, 1024)         (4,194,304 params)
 13. model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.up_proj.weight (4096, 1024)         (4,194,304 params)
 14. model.paligemma_with_expert.gemma_expert.model.layers.0.post_attention_layernorm.dense.bias (3072,)              (3,072 params)
 15. model.paligemma_with_expert.gemma_expert.model.layers.0.post_attention_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
 16. model.paligemma_with_expert.gemma_expert.model.layers.0.self_attn.k_proj.weight (256, 1024)          (262,144 params)
 17. model.paligemma_with_expert.gemma_expert.model.layers.0.self_attn.o_proj.weight (1024, 2048)         (2,097,152 params)
 18. model.paligemma_with_expert.gemma_expert.model.layers.0.self_attn.q_proj.weight (2048, 1024)         (2,097,152 params)
 19. model.paligemma_with_expert.gemma_expert.model.layers.0.self_attn.v_proj.weight (256, 1024)          (262,144 params)
 20. model.paligemma_with_expert.gemma_expert.model.layers.1.input_layernorm.dense.bias (3072,)              (3,072 params)
 21. model.paligemma_with_expert.gemma_expert.model.layers.1.input_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
 22. model.paligemma_with_expert.gemma_expert.model.layers.1.mlp.down_proj.weight (1024, 4096)         (4,194,304 params)
 23. model.paligemma_with_expert.gemma_expert.model.layers.1.mlp.gate_proj.weight (4096, 1024)         (4,194,304 params)
 24. model.paligemma_with_expert.gemma_expert.model.layers.1.mlp.up_proj.weight (4096, 1024)         (4,194,304 params)
 25. model.paligemma_with_expert.gemma_expert.model.layers.1.post_attention_layernorm.dense.bias (3072,)              (3,072 params)
 26. model.paligemma_with_expert.gemma_expert.model.layers.1.post_attention_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
 27. model.paligemma_with_expert.gemma_expert.model.layers.1.self_attn.k_proj.weight (256, 1024)          (262,144 params)
 28. model.paligemma_with_expert.gemma_expert.model.layers.1.self_attn.o_proj.weight (1024, 2048)         (2,097,152 params)
 29. model.paligemma_with_expert.gemma_expert.model.layers.1.self_attn.q_proj.weight (2048, 1024)         (2,097,152 params)
 30. model.paligemma_with_expert.gemma_expert.model.layers.1.self_attn.v_proj.weight (256, 1024)          (262,144 params)
 31. model.paligemma_with_expert.gemma_expert.model.layers.10.input_layernorm.dense.bias (3072,)              (3,072 params)
 32. model.paligemma_with_expert.gemma_expert.model.layers.10.input_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
 33. model.paligemma_with_expert.gemma_expert.model.layers.10.mlp.down_proj.weight (1024, 4096)         (4,194,304 params)
 34. model.paligemma_with_expert.gemma_expert.model.layers.10.mlp.gate_proj.weight (4096, 1024)         (4,194,304 params)
 35. model.paligemma_with_expert.gemma_expert.model.layers.10.mlp.up_proj.weight (4096, 1024)         (4,194,304 params)
 36. model.paligemma_with_expert.gemma_expert.model.layers.10.post_attention_layernorm.dense.bias (3072,)              (3,072 params)
 37. model.paligemma_with_expert.gemma_expert.model.layers.10.post_attention_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
 38. model.paligemma_with_expert.gemma_expert.model.layers.10.self_attn.k_proj.weight (256, 1024)          (262,144 params)
 39. model.paligemma_with_expert.gemma_expert.model.layers.10.self_attn.o_proj.weight (1024, 2048)         (2,097,152 params)
 40. model.paligemma_with_expert.gemma_expert.model.layers.10.self_attn.q_proj.weight (2048, 1024)         (2,097,152 params)
 41. model.paligemma_with_expert.gemma_expert.model.layers.10.self_attn.v_proj.weight (256, 1024)          (262,144 params)
 42. model.paligemma_with_expert.gemma_expert.model.layers.11.input_layernorm.dense.bias (3072,)              (3,072 params)
 43. model.paligemma_with_expert.gemma_expert.model.layers.11.input_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
 44. model.paligemma_with_expert.gemma_expert.model.layers.11.mlp.down_proj.weight (1024, 4096)         (4,194,304 params)
 45. model.paligemma_with_expert.gemma_expert.model.layers.11.mlp.gate_proj.weight (4096, 1024)         (4,194,304 params)
 46. model.paligemma_with_expert.gemma_expert.model.layers.11.mlp.up_proj.weight (4096, 1024)         (4,194,304 params)
 47. model.paligemma_with_expert.gemma_expert.model.layers.11.post_attention_layernorm.dense.bias (3072,)              (3,072 params)
 48. model.paligemma_with_expert.gemma_expert.model.layers.11.post_attention_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
 49. model.paligemma_with_expert.gemma_expert.model.layers.11.self_attn.k_proj.weight (256, 1024)          (262,144 params)
 50. model.paligemma_with_expert.gemma_expert.model.layers.11.self_attn.o_proj.weight (1024, 2048)         (2,097,152 params)
 51. model.paligemma_with_expert.gemma_expert.model.layers.11.self_attn.q_proj.weight (2048, 1024)         (2,097,152 params)
 52. model.paligemma_with_expert.gemma_expert.model.layers.11.self_attn.v_proj.weight (256, 1024)          (262,144 params)
 53. model.paligemma_with_expert.gemma_expert.model.layers.12.input_layernorm.dense.bias (3072,)              (3,072 params)
 54. model.paligemma_with_expert.gemma_expert.model.layers.12.input_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
 55. model.paligemma_with_expert.gemma_expert.model.layers.12.mlp.down_proj.weight (1024, 4096)         (4,194,304 params)
 56. model.paligemma_with_expert.gemma_expert.model.layers.12.mlp.gate_proj.weight (4096, 1024)         (4,194,304 params)
 57. model.paligemma_with_expert.gemma_expert.model.layers.12.mlp.up_proj.weight (4096, 1024)         (4,194,304 params)
 58. model.paligemma_with_expert.gemma_expert.model.layers.12.post_attention_layernorm.dense.bias (3072,)              (3,072 params)
 59. model.paligemma_with_expert.gemma_expert.model.layers.12.post_attention_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
 60. model.paligemma_with_expert.gemma_expert.model.layers.12.self_attn.k_proj.weight (256, 1024)          (262,144 params)
 61. model.paligemma_with_expert.gemma_expert.model.layers.12.self_attn.o_proj.weight (1024, 2048)         (2,097,152 params)
 62. model.paligemma_with_expert.gemma_expert.model.layers.12.self_attn.q_proj.weight (2048, 1024)         (2,097,152 params)
 63. model.paligemma_with_expert.gemma_expert.model.layers.12.self_attn.v_proj.weight (256, 1024)          (262,144 params)
 64. model.paligemma_with_expert.gemma_expert.model.layers.13.input_layernorm.dense.bias (3072,)              (3,072 params)
 65. model.paligemma_with_expert.gemma_expert.model.layers.13.input_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
 66. model.paligemma_with_expert.gemma_expert.model.layers.13.mlp.down_proj.weight (1024, 4096)         (4,194,304 params)
 67. model.paligemma_with_expert.gemma_expert.model.layers.13.mlp.gate_proj.weight (4096, 1024)         (4,194,304 params)
 68. model.paligemma_with_expert.gemma_expert.model.layers.13.mlp.up_proj.weight (4096, 1024)         (4,194,304 params)
 69. model.paligemma_with_expert.gemma_expert.model.layers.13.post_attention_layernorm.dense.bias (3072,)              (3,072 params)
 70. model.paligemma_with_expert.gemma_expert.model.layers.13.post_attention_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
 71. model.paligemma_with_expert.gemma_expert.model.layers.13.self_attn.k_proj.weight (256, 1024)          (262,144 params)
 72. model.paligemma_with_expert.gemma_expert.model.layers.13.self_attn.o_proj.weight (1024, 2048)         (2,097,152 params)
 73. model.paligemma_with_expert.gemma_expert.model.layers.13.self_attn.q_proj.weight (2048, 1024)         (2,097,152 params)
 74. model.paligemma_with_expert.gemma_expert.model.layers.13.self_attn.v_proj.weight (256, 1024)          (262,144 params)
 75. model.paligemma_with_expert.gemma_expert.model.layers.14.input_layernorm.dense.bias (3072,)              (3,072 params)
 76. model.paligemma_with_expert.gemma_expert.model.layers.14.input_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
 77. model.paligemma_with_expert.gemma_expert.model.layers.14.mlp.down_proj.weight (1024, 4096)         (4,194,304 params)
 78. model.paligemma_with_expert.gemma_expert.model.layers.14.mlp.gate_proj.weight (4096, 1024)         (4,194,304 params)
 79. model.paligemma_with_expert.gemma_expert.model.layers.14.mlp.up_proj.weight (4096, 1024)         (4,194,304 params)
 80. model.paligemma_with_expert.gemma_expert.model.layers.14.post_attention_layernorm.dense.bias (3072,)              (3,072 params)
 81. model.paligemma_with_expert.gemma_expert.model.layers.14.post_attention_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
 82. model.paligemma_with_expert.gemma_expert.model.layers.14.self_attn.k_proj.weight (256, 1024)          (262,144 params)
 83. model.paligemma_with_expert.gemma_expert.model.layers.14.self_attn.o_proj.weight (1024, 2048)         (2,097,152 params)
 84. model.paligemma_with_expert.gemma_expert.model.layers.14.self_attn.q_proj.weight (2048, 1024)         (2,097,152 params)
 85. model.paligemma_with_expert.gemma_expert.model.layers.14.self_attn.v_proj.weight (256, 1024)          (262,144 params)
 86. model.paligemma_with_expert.gemma_expert.model.layers.15.input_layernorm.dense.bias (3072,)              (3,072 params)
 87. model.paligemma_with_expert.gemma_expert.model.layers.15.input_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
 88. model.paligemma_with_expert.gemma_expert.model.layers.15.mlp.down_proj.weight (1024, 4096)         (4,194,304 params)
 89. model.paligemma_with_expert.gemma_expert.model.layers.15.mlp.gate_proj.weight (4096, 1024)         (4,194,304 params)
 90. model.paligemma_with_expert.gemma_expert.model.layers.15.mlp.up_proj.weight (4096, 1024)         (4,194,304 params)
 91. model.paligemma_with_expert.gemma_expert.model.layers.15.post_attention_layernorm.dense.bias (3072,)              (3,072 params)
 92. model.paligemma_with_expert.gemma_expert.model.layers.15.post_attention_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
 93. model.paligemma_with_expert.gemma_expert.model.layers.15.self_attn.k_proj.weight (256, 1024)          (262,144 params)
 94. model.paligemma_with_expert.gemma_expert.model.layers.15.self_attn.o_proj.weight (1024, 2048)         (2,097,152 params)
 95. model.paligemma_with_expert.gemma_expert.model.layers.15.self_attn.q_proj.weight (2048, 1024)         (2,097,152 params)
 96. model.paligemma_with_expert.gemma_expert.model.layers.15.self_attn.v_proj.weight (256, 1024)          (262,144 params)
 97. model.paligemma_with_expert.gemma_expert.model.layers.16.input_layernorm.dense.bias (3072,)              (3,072 params)
 98. model.paligemma_with_expert.gemma_expert.model.layers.16.input_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
 99. model.paligemma_with_expert.gemma_expert.model.layers.16.mlp.down_proj.weight (1024, 4096)         (4,194,304 params)
100. model.paligemma_with_expert.gemma_expert.model.layers.16.mlp.gate_proj.weight (4096, 1024)         (4,194,304 params)
101. model.paligemma_with_expert.gemma_expert.model.layers.16.mlp.up_proj.weight (4096, 1024)         (4,194,304 params)
102. model.paligemma_with_expert.gemma_expert.model.layers.16.post_attention_layernorm.dense.bias (3072,)              (3,072 params)
103. model.paligemma_with_expert.gemma_expert.model.layers.16.post_attention_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
104. model.paligemma_with_expert.gemma_expert.model.layers.16.self_attn.k_proj.weight (256, 1024)          (262,144 params)
105. model.paligemma_with_expert.gemma_expert.model.layers.16.self_attn.o_proj.weight (1024, 2048)         (2,097,152 params)
106. model.paligemma_with_expert.gemma_expert.model.layers.16.self_attn.q_proj.weight (2048, 1024)         (2,097,152 params)
107. model.paligemma_with_expert.gemma_expert.model.layers.16.self_attn.v_proj.weight (256, 1024)          (262,144 params)
108. model.paligemma_with_expert.gemma_expert.model.layers.17.input_layernorm.dense.bias (3072,)              (3,072 params)
109. model.paligemma_with_expert.gemma_expert.model.layers.17.input_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
110. model.paligemma_with_expert.gemma_expert.model.layers.17.mlp.down_proj.weight (1024, 4096)         (4,194,304 params)
111. model.paligemma_with_expert.gemma_expert.model.layers.17.mlp.gate_proj.weight (4096, 1024)         (4,194,304 params)
112. model.paligemma_with_expert.gemma_expert.model.layers.17.mlp.up_proj.weight (4096, 1024)         (4,194,304 params)
113. model.paligemma_with_expert.gemma_expert.model.layers.17.post_attention_layernorm.dense.bias (3072,)              (3,072 params)
114. model.paligemma_with_expert.gemma_expert.model.layers.17.post_attention_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
115. model.paligemma_with_expert.gemma_expert.model.layers.17.self_attn.k_proj.weight (256, 1024)          (262,144 params)
116. model.paligemma_with_expert.gemma_expert.model.layers.17.self_attn.o_proj.weight (1024, 2048)         (2,097,152 params)
117. model.paligemma_with_expert.gemma_expert.model.layers.17.self_attn.q_proj.weight (2048, 1024)         (2,097,152 params)
118. model.paligemma_with_expert.gemma_expert.model.layers.17.self_attn.v_proj.weight (256, 1024)          (262,144 params)
119. model.paligemma_with_expert.gemma_expert.model.layers.2.input_layernorm.dense.bias (3072,)              (3,072 params)
120. model.paligemma_with_expert.gemma_expert.model.layers.2.input_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
121. model.paligemma_with_expert.gemma_expert.model.layers.2.mlp.down_proj.weight (1024, 4096)         (4,194,304 params)
122. model.paligemma_with_expert.gemma_expert.model.layers.2.mlp.gate_proj.weight (4096, 1024)         (4,194,304 params)
123. model.paligemma_with_expert.gemma_expert.model.layers.2.mlp.up_proj.weight (4096, 1024)         (4,194,304 params)
124. model.paligemma_with_expert.gemma_expert.model.layers.2.post_attention_layernorm.dense.bias (3072,)              (3,072 params)
125. model.paligemma_with_expert.gemma_expert.model.layers.2.post_attention_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
126. model.paligemma_with_expert.gemma_expert.model.layers.2.self_attn.k_proj.weight (256, 1024)          (262,144 params)
127. model.paligemma_with_expert.gemma_expert.model.layers.2.self_attn.o_proj.weight (1024, 2048)         (2,097,152 params)
128. model.paligemma_with_expert.gemma_expert.model.layers.2.self_attn.q_proj.weight (2048, 1024)         (2,097,152 params)
129. model.paligemma_with_expert.gemma_expert.model.layers.2.self_attn.v_proj.weight (256, 1024)          (262,144 params)
130. model.paligemma_with_expert.gemma_expert.model.layers.3.input_layernorm.dense.bias (3072,)              (3,072 params)
131. model.paligemma_with_expert.gemma_expert.model.layers.3.input_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
132. model.paligemma_with_expert.gemma_expert.model.layers.3.mlp.down_proj.weight (1024, 4096)         (4,194,304 params)
133. model.paligemma_with_expert.gemma_expert.model.layers.3.mlp.gate_proj.weight (4096, 1024)         (4,194,304 params)
134. model.paligemma_with_expert.gemma_expert.model.layers.3.mlp.up_proj.weight (4096, 1024)         (4,194,304 params)
135. model.paligemma_with_expert.gemma_expert.model.layers.3.post_attention_layernorm.dense.bias (3072,)              (3,072 params)
136. model.paligemma_with_expert.gemma_expert.model.layers.3.post_attention_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
137. model.paligemma_with_expert.gemma_expert.model.layers.3.self_attn.k_proj.weight (256, 1024)          (262,144 params)
138. model.paligemma_with_expert.gemma_expert.model.layers.3.self_attn.o_proj.weight (1024, 2048)         (2,097,152 params)
139. model.paligemma_with_expert.gemma_expert.model.layers.3.self_attn.q_proj.weight (2048, 1024)         (2,097,152 params)
140. model.paligemma_with_expert.gemma_expert.model.layers.3.self_attn.v_proj.weight (256, 1024)          (262,144 params)
141. model.paligemma_with_expert.gemma_expert.model.layers.4.input_layernorm.dense.bias (3072,)              (3,072 params)
142. model.paligemma_with_expert.gemma_expert.model.layers.4.input_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
143. model.paligemma_with_expert.gemma_expert.model.layers.4.mlp.down_proj.weight (1024, 4096)         (4,194,304 params)
144. model.paligemma_with_expert.gemma_expert.model.layers.4.mlp.gate_proj.weight (4096, 1024)         (4,194,304 params)
145. model.paligemma_with_expert.gemma_expert.model.layers.4.mlp.up_proj.weight (4096, 1024)         (4,194,304 params)
146. model.paligemma_with_expert.gemma_expert.model.layers.4.post_attention_layernorm.dense.bias (3072,)              (3,072 params)
147. model.paligemma_with_expert.gemma_expert.model.layers.4.post_attention_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
148. model.paligemma_with_expert.gemma_expert.model.layers.4.self_attn.k_proj.weight (256, 1024)          (262,144 params)
149. model.paligemma_with_expert.gemma_expert.model.layers.4.self_attn.o_proj.weight (1024, 2048)         (2,097,152 params)
150. model.paligemma_with_expert.gemma_expert.model.layers.4.self_attn.q_proj.weight (2048, 1024)         (2,097,152 params)
151. model.paligemma_with_expert.gemma_expert.model.layers.4.self_attn.v_proj.weight (256, 1024)          (262,144 params)
152. model.paligemma_with_expert.gemma_expert.model.layers.5.input_layernorm.dense.bias (3072,)              (3,072 params)
153. model.paligemma_with_expert.gemma_expert.model.layers.5.input_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
154. model.paligemma_with_expert.gemma_expert.model.layers.5.mlp.down_proj.weight (1024, 4096)         (4,194,304 params)
155. model.paligemma_with_expert.gemma_expert.model.layers.5.mlp.gate_proj.weight (4096, 1024)         (4,194,304 params)
156. model.paligemma_with_expert.gemma_expert.model.layers.5.mlp.up_proj.weight (4096, 1024)         (4,194,304 params)
157. model.paligemma_with_expert.gemma_expert.model.layers.5.post_attention_layernorm.dense.bias (3072,)              (3,072 params)
158. model.paligemma_with_expert.gemma_expert.model.layers.5.post_attention_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
159. model.paligemma_with_expert.gemma_expert.model.layers.5.self_attn.k_proj.weight (256, 1024)          (262,144 params)
160. model.paligemma_with_expert.gemma_expert.model.layers.5.self_attn.o_proj.weight (1024, 2048)         (2,097,152 params)
161. model.paligemma_with_expert.gemma_expert.model.layers.5.self_attn.q_proj.weight (2048, 1024)         (2,097,152 params)
162. model.paligemma_with_expert.gemma_expert.model.layers.5.self_attn.v_proj.weight (256, 1024)          (262,144 params)
163. model.paligemma_with_expert.gemma_expert.model.layers.6.input_layernorm.dense.bias (3072,)              (3,072 params)
164. model.paligemma_with_expert.gemma_expert.model.layers.6.input_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
165. model.paligemma_with_expert.gemma_expert.model.layers.6.mlp.down_proj.weight (1024, 4096)         (4,194,304 params)
166. model.paligemma_with_expert.gemma_expert.model.layers.6.mlp.gate_proj.weight (4096, 1024)         (4,194,304 params)
167. model.paligemma_with_expert.gemma_expert.model.layers.6.mlp.up_proj.weight (4096, 1024)         (4,194,304 params)
168. model.paligemma_with_expert.gemma_expert.model.layers.6.post_attention_layernorm.dense.bias (3072,)              (3,072 params)
169. model.paligemma_with_expert.gemma_expert.model.layers.6.post_attention_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
170. model.paligemma_with_expert.gemma_expert.model.layers.6.self_attn.k_proj.weight (256, 1024)          (262,144 params)
171. model.paligemma_with_expert.gemma_expert.model.layers.6.self_attn.o_proj.weight (1024, 2048)         (2,097,152 params)
172. model.paligemma_with_expert.gemma_expert.model.layers.6.self_attn.q_proj.weight (2048, 1024)         (2,097,152 params)
173. model.paligemma_with_expert.gemma_expert.model.layers.6.self_attn.v_proj.weight (256, 1024)          (262,144 params)
174. model.paligemma_with_expert.gemma_expert.model.layers.7.input_layernorm.dense.bias (3072,)              (3,072 params)
175. model.paligemma_with_expert.gemma_expert.model.layers.7.input_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
176. model.paligemma_with_expert.gemma_expert.model.layers.7.mlp.down_proj.weight (1024, 4096)         (4,194,304 params)
177. model.paligemma_with_expert.gemma_expert.model.layers.7.mlp.gate_proj.weight (4096, 1024)         (4,194,304 params)
178. model.paligemma_with_expert.gemma_expert.model.layers.7.mlp.up_proj.weight (4096, 1024)         (4,194,304 params)
179. model.paligemma_with_expert.gemma_expert.model.layers.7.post_attention_layernorm.dense.bias (3072,)              (3,072 params)
180. model.paligemma_with_expert.gemma_expert.model.layers.7.post_attention_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
181. model.paligemma_with_expert.gemma_expert.model.layers.7.self_attn.k_proj.weight (256, 1024)          (262,144 params)
182. model.paligemma_with_expert.gemma_expert.model.layers.7.self_attn.o_proj.weight (1024, 2048)         (2,097,152 params)
183. model.paligemma_with_expert.gemma_expert.model.layers.7.self_attn.q_proj.weight (2048, 1024)         (2,097,152 params)
184. model.paligemma_with_expert.gemma_expert.model.layers.7.self_attn.v_proj.weight (256, 1024)          (262,144 params)
185. model.paligemma_with_expert.gemma_expert.model.layers.8.input_layernorm.dense.bias (3072,)              (3,072 params)
186. model.paligemma_with_expert.gemma_expert.model.layers.8.input_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
187. model.paligemma_with_expert.gemma_expert.model.layers.8.mlp.down_proj.weight (1024, 4096)         (4,194,304 params)
188. model.paligemma_with_expert.gemma_expert.model.layers.8.mlp.gate_proj.weight (4096, 1024)         (4,194,304 params)
189. model.paligemma_with_expert.gemma_expert.model.layers.8.mlp.up_proj.weight (4096, 1024)         (4,194,304 params)
190. model.paligemma_with_expert.gemma_expert.model.layers.8.post_attention_layernorm.dense.bias (3072,)              (3,072 params)
191. model.paligemma_with_expert.gemma_expert.model.layers.8.post_attention_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
192. model.paligemma_with_expert.gemma_expert.model.layers.8.self_attn.k_proj.weight (256, 1024)          (262,144 params)
193. model.paligemma_with_expert.gemma_expert.model.layers.8.self_attn.o_proj.weight (1024, 2048)         (2,097,152 params)
194. model.paligemma_with_expert.gemma_expert.model.layers.8.self_attn.q_proj.weight (2048, 1024)         (2,097,152 params)
195. model.paligemma_with_expert.gemma_expert.model.layers.8.self_attn.v_proj.weight (256, 1024)          (262,144 params)
196. model.paligemma_with_expert.gemma_expert.model.layers.9.input_layernorm.dense.bias (3072,)              (3,072 params)
197. model.paligemma_with_expert.gemma_expert.model.layers.9.input_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
198. model.paligemma_with_expert.gemma_expert.model.layers.9.mlp.down_proj.weight (1024, 4096)         (4,194,304 params)
199. model.paligemma_with_expert.gemma_expert.model.layers.9.mlp.gate_proj.weight (4096, 1024)         (4,194,304 params)
200. model.paligemma_with_expert.gemma_expert.model.layers.9.mlp.up_proj.weight (4096, 1024)         (4,194,304 params)
201. model.paligemma_with_expert.gemma_expert.model.layers.9.post_attention_layernorm.dense.bias (3072,)              (3,072 params)
202. model.paligemma_with_expert.gemma_expert.model.layers.9.post_attention_layernorm.dense.weight (3072, 1024)         (3,145,728 params)
203. model.paligemma_with_expert.gemma_expert.model.layers.9.self_attn.k_proj.weight (256, 1024)          (262,144 params)
204. model.paligemma_with_expert.gemma_expert.model.layers.9.self_attn.o_proj.weight (1024, 2048)         (2,097,152 params)
205. model.paligemma_with_expert.gemma_expert.model.layers.9.self_attn.q_proj.weight (2048, 1024)         (2,097,152 params)
206. model.paligemma_with_expert.gemma_expert.model.layers.9.self_attn.v_proj.weight (256, 1024)          (262,144 params)
207. model.paligemma_with_expert.gemma_expert.model.norm.dense.bias (3072,)              (3,072 params)
208. model.paligemma_with_expert.gemma_expert.model.norm.dense.weight (3072, 1024)         (3,145,728 params)
209. model.paligemma_with_expert.paligemma.lm_head.weight (257152, 2048)       (526,647,296 params)
210. model.paligemma_with_expert.paligemma.model.language_model.embed_tokens.weight (257152, 2048)       (526,647,296 params)
211. model.paligemma_with_expert.paligemma.model.language_model.layers.0.input_layernorm.weight (2048,)              (2,048 params)
212. model.paligemma_with_expert.paligemma.model.language_model.layers.0.mlp.down_proj.weight (2048, 16384)        (33,554,432 params)
213. model.paligemma_with_expert.paligemma.model.language_model.layers.0.mlp.gate_proj.weight (16384, 2048)        (33,554,432 params)
214. model.paligemma_with_expert.paligemma.model.language_model.layers.0.mlp.up_proj.weight (16384, 2048)        (33,554,432 params)
215. model.paligemma_with_expert.paligemma.model.language_model.layers.0.post_attention_layernorm.weight (2048,)              (2,048 params)
216. model.paligemma_with_expert.paligemma.model.language_model.layers.0.self_attn.k_proj.weight (256, 2048)          (524,288 params)
217. model.paligemma_with_expert.paligemma.model.language_model.layers.0.self_attn.o_proj.weight (2048, 2048)         (4,194,304 params)
218. model.paligemma_with_expert.paligemma.model.language_model.layers.0.self_attn.q_proj.weight (2048, 2048)         (4,194,304 params)
219. model.paligemma_with_expert.paligemma.model.language_model.layers.0.self_attn.v_proj.weight (256, 2048)          (524,288 params)
220. model.paligemma_with_expert.paligemma.model.language_model.layers.1.input_layernorm.weight (2048,)              (2,048 params)
221. model.paligemma_with_expert.paligemma.model.language_model.layers.1.mlp.down_proj.weight (2048, 16384)        (33,554,432 params)
222. model.paligemma_with_expert.paligemma.model.language_model.layers.1.mlp.gate_proj.weight (16384, 2048)        (33,554,432 params)
223. model.paligemma_with_expert.paligemma.model.language_model.layers.1.mlp.up_proj.weight (16384, 2048)        (33,554,432 params)
224. model.paligemma_with_expert.paligemma.model.language_model.layers.1.post_attention_layernorm.weight (2048,)              (2,048 params)
225. model.paligemma_with_expert.paligemma.model.language_model.layers.1.self_attn.k_proj.weight (256, 2048)          (524,288 params)
226. model.paligemma_with_expert.paligemma.model.language_model.layers.1.self_attn.o_proj.weight (2048, 2048)         (4,194,304 params)
227. model.paligemma_with_expert.paligemma.model.language_model.layers.1.self_attn.q_proj.weight (2048, 2048)         (4,194,304 params)
228. model.paligemma_with_expert.paligemma.model.language_model.layers.1.self_attn.v_proj.weight (256, 2048)          (524,288 params)
229. model.paligemma_with_expert.paligemma.model.language_model.layers.10.input_layernorm.weight (2048,)              (2,048 params)
230. model.paligemma_with_expert.paligemma.model.language_model.layers.10.mlp.down_proj.weight (2048, 16384)        (33,554,432 params)
231. model.paligemma_with_expert.paligemma.model.language_model.layers.10.mlp.gate_proj.weight (16384, 2048)        (33,554,432 params)
232. model.paligemma_with_expert.paligemma.model.language_model.layers.10.mlp.up_proj.weight (16384, 2048)        (33,554,432 params)
233. model.paligemma_with_expert.paligemma.model.language_model.layers.10.post_attention_layernorm.weight (2048,)              (2,048 params)
234. model.paligemma_with_expert.paligemma.model.language_model.layers.10.self_attn.k_proj.weight (256, 2048)          (524,288 params)
235. model.paligemma_with_expert.paligemma.model.language_model.layers.10.self_attn.o_proj.weight (2048, 2048)         (4,194,304 params)
236. model.paligemma_with_expert.paligemma.model.language_model.layers.10.self_attn.q_proj.weight (2048, 2048)         (4,194,304 params)
237. model.paligemma_with_expert.paligemma.model.language_model.layers.10.self_attn.v_proj.weight (256, 2048)          (524,288 params)
238. model.paligemma_with_expert.paligemma.model.language_model.layers.11.input_layernorm.weight (2048,)              (2,048 params)
239. model.paligemma_with_expert.paligemma.model.language_model.layers.11.mlp.down_proj.weight (2048, 16384)        (33,554,432 params)
240. model.paligemma_with_expert.paligemma.model.language_model.layers.11.mlp.gate_proj.weight (16384, 2048)        (33,554,432 params)
241. model.paligemma_with_expert.paligemma.model.language_model.layers.11.mlp.up_proj.weight (16384, 2048)        (33,554,432 params)
242. model.paligemma_with_expert.paligemma.model.language_model.layers.11.post_attention_layernorm.weight (2048,)              (2,048 params)
243. model.paligemma_with_expert.paligemma.model.language_model.layers.11.self_attn.k_proj.weight (256, 2048)          (524,288 params)
244. model.paligemma_with_expert.paligemma.model.language_model.layers.11.self_attn.o_proj.weight (2048, 2048)         (4,194,304 params)
245. model.paligemma_with_expert.paligemma.model.language_model.layers.11.self_attn.q_proj.weight (2048, 2048)         (4,194,304 params)
246. model.paligemma_with_expert.paligemma.model.language_model.layers.11.self_attn.v_proj.weight (256, 2048)          (524,288 params)
247. model.paligemma_with_expert.paligemma.model.language_model.layers.12.input_layernorm.weight (2048,)              (2,048 params)
248. model.paligemma_with_expert.paligemma.model.language_model.layers.12.mlp.down_proj.weight (2048, 16384)        (33,554,432 params)
249. model.paligemma_with_expert.paligemma.model.language_model.layers.12.mlp.gate_proj.weight (16384, 2048)        (33,554,432 params)
250. model.paligemma_with_expert.paligemma.model.language_model.layers.12.mlp.up_proj.weight (16384, 2048)        (33,554,432 params)
251. model.paligemma_with_expert.paligemma.model.language_model.layers.12.post_attention_layernorm.weight (2048,)              (2,048 params)
252. model.paligemma_with_expert.paligemma.model.language_model.layers.12.self_attn.k_proj.weight (256, 2048)          (524,288 params)
253. model.paligemma_with_expert.paligemma.model.language_model.layers.12.self_attn.o_proj.weight (2048, 2048)         (4,194,304 params)
254. model.paligemma_with_expert.paligemma.model.language_model.layers.12.self_attn.q_proj.weight (2048, 2048)         (4,194,304 params)
255. model.paligemma_with_expert.paligemma.model.language_model.layers.12.self_attn.v_proj.weight (256, 2048)          (524,288 params)
256. model.paligemma_with_expert.paligemma.model.language_model.layers.13.input_layernorm.weight (2048,)              (2,048 params)
257. model.paligemma_with_expert.paligemma.model.language_model.layers.13.mlp.down_proj.weight (2048, 16384)        (33,554,432 params)
258. model.paligemma_with_expert.paligemma.model.language_model.layers.13.mlp.gate_proj.weight (16384, 2048)        (33,554,432 params)
259. model.paligemma_with_expert.paligemma.model.language_model.layers.13.mlp.up_proj.weight (16384, 2048)        (33,554,432 params)
260. model.paligemma_with_expert.paligemma.model.language_model.layers.13.post_attention_layernorm.weight (2048,)              (2,048 params)
261. model.paligemma_with_expert.paligemma.model.language_model.layers.13.self_attn.k_proj.weight (256, 2048)          (524,288 params)
262. model.paligemma_with_expert.paligemma.model.language_model.layers.13.self_attn.o_proj.weight (2048, 2048)         (4,194,304 params)
263. model.paligemma_with_expert.paligemma.model.language_model.layers.13.self_attn.q_proj.weight (2048, 2048)         (4,194,304 params)
264. model.paligemma_with_expert.paligemma.model.language_model.layers.13.self_attn.v_proj.weight (256, 2048)          (524,288 params)
265. model.paligemma_with_expert.paligemma.model.language_model.layers.14.input_layernorm.weight (2048,)              (2,048 params)
266. model.paligemma_with_expert.paligemma.model.language_model.layers.14.mlp.down_proj.weight (2048, 16384)        (33,554,432 params)
267. model.paligemma_with_expert.paligemma.model.language_model.layers.14.mlp.gate_proj.weight (16384, 2048)        (33,554,432 params)
268. model.paligemma_with_expert.paligemma.model.language_model.layers.14.mlp.up_proj.weight (16384, 2048)        (33,554,432 params)
269. model.paligemma_with_expert.paligemma.model.language_model.layers.14.post_attention_layernorm.weight (2048,)              (2,048 params)
270. model.paligemma_with_expert.paligemma.model.language_model.layers.14.self_attn.k_proj.weight (256, 2048)          (524,288 params)
271. model.paligemma_with_expert.paligemma.model.language_model.layers.14.self_attn.o_proj.weight (2048, 2048)         (4,194,304 params)
272. model.paligemma_with_expert.paligemma.model.language_model.layers.14.self_attn.q_proj.weight (2048, 2048)         (4,194,304 params)
273. model.paligemma_with_expert.paligemma.model.language_model.layers.14.self_attn.v_proj.weight (256, 2048)          (524,288 params)
274. model.paligemma_with_expert.paligemma.model.language_model.layers.15.input_layernorm.weight (2048,)              (2,048 params)
275. model.paligemma_with_expert.paligemma.model.language_model.layers.15.mlp.down_proj.weight (2048, 16384)        (33,554,432 params)
276. model.paligemma_with_expert.paligemma.model.language_model.layers.15.mlp.gate_proj.weight (16384, 2048)        (33,554,432 params)
277. model.paligemma_with_expert.paligemma.model.language_model.layers.15.mlp.up_proj.weight (16384, 2048)        (33,554,432 params)
278. model.paligemma_with_expert.paligemma.model.language_model.layers.15.post_attention_layernorm.weight (2048,)              (2,048 params)
279. model.paligemma_with_expert.paligemma.model.language_model.layers.15.self_attn.k_proj.weight (256, 2048)          (524,288 params)
280. model.paligemma_with_expert.paligemma.model.language_model.layers.15.self_attn.o_proj.weight (2048, 2048)         (4,194,304 params)
281. model.paligemma_with_expert.paligemma.model.language_model.layers.15.self_attn.q_proj.weight (2048, 2048)         (4,194,304 params)
282. model.paligemma_with_expert.paligemma.model.language_model.layers.15.self_attn.v_proj.weight (256, 2048)          (524,288 params)
283. model.paligemma_with_expert.paligemma.model.language_model.layers.16.input_layernorm.weight (2048,)              (2,048 params)
284. model.paligemma_with_expert.paligemma.model.language_model.layers.16.mlp.down_proj.weight (2048, 16384)        (33,554,432 params)
285. model.paligemma_with_expert.paligemma.model.language_model.layers.16.mlp.gate_proj.weight (16384, 2048)        (33,554,432 params)
286. model.paligemma_with_expert.paligemma.model.language_model.layers.16.mlp.up_proj.weight (16384, 2048)        (33,554,432 params)
287. model.paligemma_with_expert.paligemma.model.language_model.layers.16.post_attention_layernorm.weight (2048,)              (2,048 params)
288. model.paligemma_with_expert.paligemma.model.language_model.layers.16.self_attn.k_proj.weight (256, 2048)          (524,288 params)
289. model.paligemma_with_expert.paligemma.model.language_model.layers.16.self_attn.o_proj.weight (2048, 2048)         (4,194,304 params)
290. model.paligemma_with_expert.paligemma.model.language_model.layers.16.self_attn.q_proj.weight (2048, 2048)         (4,194,304 params)
291. model.paligemma_with_expert.paligemma.model.language_model.layers.16.self_attn.v_proj.weight (256, 2048)          (524,288 params)
292. model.paligemma_with_expert.paligemma.model.language_model.layers.17.input_layernorm.weight (2048,)              (2,048 params)
293. model.paligemma_with_expert.paligemma.model.language_model.layers.17.mlp.down_proj.weight (2048, 16384)        (33,554,432 params)
294. model.paligemma_with_expert.paligemma.model.language_model.layers.17.mlp.gate_proj.weight (16384, 2048)        (33,554,432 params)
295. model.paligemma_with_expert.paligemma.model.language_model.layers.17.mlp.up_proj.weight (16384, 2048)        (33,554,432 params)
296. model.paligemma_with_expert.paligemma.model.language_model.layers.17.post_attention_layernorm.weight (2048,)              (2,048 params)
297. model.paligemma_with_expert.paligemma.model.language_model.layers.17.self_attn.k_proj.weight (256, 2048)          (524,288 params)
298. model.paligemma_with_expert.paligemma.model.language_model.layers.17.self_attn.o_proj.weight (2048, 2048)         (4,194,304 params)
299. model.paligemma_with_expert.paligemma.model.language_model.layers.17.self_attn.q_proj.weight (2048, 2048)         (4,194,304 params)
300. model.paligemma_with_expert.paligemma.model.language_model.layers.17.self_attn.v_proj.weight (256, 2048)          (524,288 params)
301. model.paligemma_with_expert.paligemma.model.language_model.layers.2.input_layernorm.weight (2048,)              (2,048 params)
302. model.paligemma_with_expert.paligemma.model.language_model.layers.2.mlp.down_proj.weight (2048, 16384)        (33,554,432 params)
303. model.paligemma_with_expert.paligemma.model.language_model.layers.2.mlp.gate_proj.weight (16384, 2048)        (33,554,432 params)
304. model.paligemma_with_expert.paligemma.model.language_model.layers.2.mlp.up_proj.weight (16384, 2048)        (33,554,432 params)
305. model.paligemma_with_expert.paligemma.model.language_model.layers.2.post_attention_layernorm.weight (2048,)              (2,048 params)
306. model.paligemma_with_expert.paligemma.model.language_model.layers.2.self_attn.k_proj.weight (256, 2048)          (524,288 params)
307. model.paligemma_with_expert.paligemma.model.language_model.layers.2.self_attn.o_proj.weight (2048, 2048)         (4,194,304 params)
308. model.paligemma_with_expert.paligemma.model.language_model.layers.2.self_attn.q_proj.weight (2048, 2048)         (4,194,304 params)
309. model.paligemma_with_expert.paligemma.model.language_model.layers.2.self_attn.v_proj.weight (256, 2048)          (524,288 params)
310. model.paligemma_with_expert.paligemma.model.language_model.layers.3.input_layernorm.weight (2048,)              (2,048 params)
311. model.paligemma_with_expert.paligemma.model.language_model.layers.3.mlp.down_proj.weight (2048, 16384)        (33,554,432 params)
312. model.paligemma_with_expert.paligemma.model.language_model.layers.3.mlp.gate_proj.weight (16384, 2048)        (33,554,432 params)
313. model.paligemma_with_expert.paligemma.model.language_model.layers.3.mlp.up_proj.weight (16384, 2048)        (33,554,432 params)
314. model.paligemma_with_expert.paligemma.model.language_model.layers.3.post_attention_layernorm.weight (2048,)              (2,048 params)
315. model.paligemma_with_expert.paligemma.model.language_model.layers.3.self_attn.k_proj.weight (256, 2048)          (524,288 params)
316. model.paligemma_with_expert.paligemma.model.language_model.layers.3.self_attn.o_proj.weight (2048, 2048)         (4,194,304 params)
317. model.paligemma_with_expert.paligemma.model.language_model.layers.3.self_attn.q_proj.weight (2048, 2048)         (4,194,304 params)
318. model.paligemma_with_expert.paligemma.model.language_model.layers.3.self_attn.v_proj.weight (256, 2048)          (524,288 params)
319. model.paligemma_with_expert.paligemma.model.language_model.layers.4.input_layernorm.weight (2048,)              (2,048 params)
320. model.paligemma_with_expert.paligemma.model.language_model.layers.4.mlp.down_proj.weight (2048, 16384)        (33,554,432 params)
321. model.paligemma_with_expert.paligemma.model.language_model.layers.4.mlp.gate_proj.weight (16384, 2048)        (33,554,432 params)
322. model.paligemma_with_expert.paligemma.model.language_model.layers.4.mlp.up_proj.weight (16384, 2048)        (33,554,432 params)
323. model.paligemma_with_expert.paligemma.model.language_model.layers.4.post_attention_layernorm.weight (2048,)              (2,048 params)
324. model.paligemma_with_expert.paligemma.model.language_model.layers.4.self_attn.k_proj.weight (256, 2048)          (524,288 params)
325. model.paligemma_with_expert.paligemma.model.language_model.layers.4.self_attn.o_proj.weight (2048, 2048)         (4,194,304 params)
326. model.paligemma_with_expert.paligemma.model.language_model.layers.4.self_attn.q_proj.weight (2048, 2048)         (4,194,304 params)
327. model.paligemma_with_expert.paligemma.model.language_model.layers.4.self_attn.v_proj.weight (256, 2048)          (524,288 params)
328. model.paligemma_with_expert.paligemma.model.language_model.layers.5.input_layernorm.weight (2048,)              (2,048 params)
329. model.paligemma_with_expert.paligemma.model.language_model.layers.5.mlp.down_proj.weight (2048, 16384)        (33,554,432 params)
330. model.paligemma_with_expert.paligemma.model.language_model.layers.5.mlp.gate_proj.weight (16384, 2048)        (33,554,432 params)
331. model.paligemma_with_expert.paligemma.model.language_model.layers.5.mlp.up_proj.weight (16384, 2048)        (33,554,432 params)
332. model.paligemma_with_expert.paligemma.model.language_model.layers.5.post_attention_layernorm.weight (2048,)              (2,048 params)
333. model.paligemma_with_expert.paligemma.model.language_model.layers.5.self_attn.k_proj.weight (256, 2048)          (524,288 params)
334. model.paligemma_with_expert.paligemma.model.language_model.layers.5.self_attn.o_proj.weight (2048, 2048)         (4,194,304 params)
335. model.paligemma_with_expert.paligemma.model.language_model.layers.5.self_attn.q_proj.weight (2048, 2048)         (4,194,304 params)
336. model.paligemma_with_expert.paligemma.model.language_model.layers.5.self_attn.v_proj.weight (256, 2048)          (524,288 params)
337. model.paligemma_with_expert.paligemma.model.language_model.layers.6.input_layernorm.weight (2048,)              (2,048 params)
338. model.paligemma_with_expert.paligemma.model.language_model.layers.6.mlp.down_proj.weight (2048, 16384)        (33,554,432 params)
339. model.paligemma_with_expert.paligemma.model.language_model.layers.6.mlp.gate_proj.weight (16384, 2048)        (33,554,432 params)
340. model.paligemma_with_expert.paligemma.model.language_model.layers.6.mlp.up_proj.weight (16384, 2048)        (33,554,432 params)
341. model.paligemma_with_expert.paligemma.model.language_model.layers.6.post_attention_layernorm.weight (2048,)              (2,048 params)
342. model.paligemma_with_expert.paligemma.model.language_model.layers.6.self_attn.k_proj.weight (256, 2048)          (524,288 params)
343. model.paligemma_with_expert.paligemma.model.language_model.layers.6.self_attn.o_proj.weight (2048, 2048)         (4,194,304 params)
344. model.paligemma_with_expert.paligemma.model.language_model.layers.6.self_attn.q_proj.weight (2048, 2048)         (4,194,304 params)
345. model.paligemma_with_expert.paligemma.model.language_model.layers.6.self_attn.v_proj.weight (256, 2048)          (524,288 params)
346. model.paligemma_with_expert.paligemma.model.language_model.layers.7.input_layernorm.weight (2048,)              (2,048 params)
347. model.paligemma_with_expert.paligemma.model.language_model.layers.7.mlp.down_proj.weight (2048, 16384)        (33,554,432 params)
348. model.paligemma_with_expert.paligemma.model.language_model.layers.7.mlp.gate_proj.weight (16384, 2048)        (33,554,432 params)
349. model.paligemma_with_expert.paligemma.model.language_model.layers.7.mlp.up_proj.weight (16384, 2048)        (33,554,432 params)
350. model.paligemma_with_expert.paligemma.model.language_model.layers.7.post_attention_layernorm.weight (2048,)              (2,048 params)
351. model.paligemma_with_expert.paligemma.model.language_model.layers.7.self_attn.k_proj.weight (256, 2048)          (524,288 params)
352. model.paligemma_with_expert.paligemma.model.language_model.layers.7.self_attn.o_proj.weight (2048, 2048)         (4,194,304 params)
353. model.paligemma_with_expert.paligemma.model.language_model.layers.7.self_attn.q_proj.weight (2048, 2048)         (4,194,304 params)
354. model.paligemma_with_expert.paligemma.model.language_model.layers.7.self_attn.v_proj.weight (256, 2048)          (524,288 params)
355. model.paligemma_with_expert.paligemma.model.language_model.layers.8.input_layernorm.weight (2048,)              (2,048 params)
356. model.paligemma_with_expert.paligemma.model.language_model.layers.8.mlp.down_proj.weight (2048, 16384)        (33,554,432 params)
357. model.paligemma_with_expert.paligemma.model.language_model.layers.8.mlp.gate_proj.weight (16384, 2048)        (33,554,432 params)
358. model.paligemma_with_expert.paligemma.model.language_model.layers.8.mlp.up_proj.weight (16384, 2048)        (33,554,432 params)
359. model.paligemma_with_expert.paligemma.model.language_model.layers.8.post_attention_layernorm.weight (2048,)              (2,048 params)
360. model.paligemma_with_expert.paligemma.model.language_model.layers.8.self_attn.k_proj.weight (256, 2048)          (524,288 params)
361. model.paligemma_with_expert.paligemma.model.language_model.layers.8.self_attn.o_proj.weight (2048, 2048)         (4,194,304 params)
362. model.paligemma_with_expert.paligemma.model.language_model.layers.8.self_attn.q_proj.weight (2048, 2048)         (4,194,304 params)
363. model.paligemma_with_expert.paligemma.model.language_model.layers.8.self_attn.v_proj.weight (256, 2048)          (524,288 params)
364. model.paligemma_with_expert.paligemma.model.language_model.layers.9.input_layernorm.weight (2048,)              (2,048 params)
365. model.paligemma_with_expert.paligemma.model.language_model.layers.9.mlp.down_proj.weight (2048, 16384)        (33,554,432 params)
366. model.paligemma_with_expert.paligemma.model.language_model.layers.9.mlp.gate_proj.weight (16384, 2048)        (33,554,432 params)
367. model.paligemma_with_expert.paligemma.model.language_model.layers.9.mlp.up_proj.weight (16384, 2048)        (33,554,432 params)
368. model.paligemma_with_expert.paligemma.model.language_model.layers.9.post_attention_layernorm.weight (2048,)              (2,048 params)
369. model.paligemma_with_expert.paligemma.model.language_model.layers.9.self_attn.k_proj.weight (256, 2048)          (524,288 params)
370. model.paligemma_with_expert.paligemma.model.language_model.layers.9.self_attn.o_proj.weight (2048, 2048)         (4,194,304 params)
371. model.paligemma_with_expert.paligemma.model.language_model.layers.9.self_attn.q_proj.weight (2048, 2048)         (4,194,304 params)
372. model.paligemma_with_expert.paligemma.model.language_model.layers.9.self_attn.v_proj.weight (256, 2048)          (524,288 params)
373. model.paligemma_with_expert.paligemma.model.language_model.norm.weight (2048,)              (2,048 params)
374. model.paligemma_with_expert.paligemma.model.multi_modal_projector.linear.bias (2048,)              (2,048 params)
375. model.paligemma_with_expert.paligemma.model.multi_modal_projector.linear.weight (2048, 1152)         (2,359,296 params)
376. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.embeddings.patch_embedding.bias (1152,)              (1,152 params)
377. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.embeddings.patch_embedding.weight (1152, 3, 14, 14)    (677,376 params)
378. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.embeddings.position_embedding.weight (256, 1152)          (294,912 params)
379. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias (1152,)              (1,152 params)
380. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight (1152,)              (1,152 params)
381. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias (1152,)              (1,152 params)
382. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight (1152,)              (1,152 params)
383. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias (4304,)              (4,304 params)
384. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
385. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias (1152,)              (1,152 params)
386. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
387. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias (1152,)              (1,152 params)
388. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
389. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias (1152,)              (1,152 params)
390. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
391. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias (1152,)              (1,152 params)
392. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
393. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias (1152,)              (1,152 params)
394. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
395. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias (1152,)              (1,152 params)
396. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight (1152,)              (1,152 params)
397. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias (1152,)              (1,152 params)
398. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight (1152,)              (1,152 params)
399. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias (4304,)              (4,304 params)
400. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
401. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias (1152,)              (1,152 params)
402. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
403. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias (1152,)              (1,152 params)
404. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
405. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias (1152,)              (1,152 params)
406. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
407. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias (1152,)              (1,152 params)
408. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
409. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias (1152,)              (1,152 params)
410. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
411. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias (1152,)              (1,152 params)
412. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight (1152,)              (1,152 params)
413. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias (1152,)              (1,152 params)
414. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight (1152,)              (1,152 params)
415. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias (4304,)              (4,304 params)
416. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
417. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias (1152,)              (1,152 params)
418. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
419. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias (1152,)              (1,152 params)
420. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
421. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias (1152,)              (1,152 params)
422. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
423. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias (1152,)              (1,152 params)
424. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
425. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias (1152,)              (1,152 params)
426. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
427. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias (1152,)              (1,152 params)
428. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight (1152,)              (1,152 params)
429. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias (1152,)              (1,152 params)
430. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight (1152,)              (1,152 params)
431. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias (4304,)              (4,304 params)
432. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
433. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias (1152,)              (1,152 params)
434. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
435. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias (1152,)              (1,152 params)
436. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
437. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias (1152,)              (1,152 params)
438. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
439. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias (1152,)              (1,152 params)
440. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
441. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias (1152,)              (1,152 params)
442. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
443. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias (1152,)              (1,152 params)
444. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight (1152,)              (1,152 params)
445. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias (1152,)              (1,152 params)
446. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight (1152,)              (1,152 params)
447. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias (4304,)              (4,304 params)
448. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
449. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias (1152,)              (1,152 params)
450. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
451. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias (1152,)              (1,152 params)
452. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
453. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias (1152,)              (1,152 params)
454. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
455. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias (1152,)              (1,152 params)
456. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
457. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias (1152,)              (1,152 params)
458. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
459. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias (1152,)              (1,152 params)
460. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight (1152,)              (1,152 params)
461. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias (1152,)              (1,152 params)
462. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight (1152,)              (1,152 params)
463. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias (4304,)              (4,304 params)
464. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
465. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias (1152,)              (1,152 params)
466. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
467. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias (1152,)              (1,152 params)
468. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
469. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias (1152,)              (1,152 params)
470. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
471. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias (1152,)              (1,152 params)
472. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
473. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias (1152,)              (1,152 params)
474. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
475. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias (1152,)              (1,152 params)
476. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight (1152,)              (1,152 params)
477. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias (1152,)              (1,152 params)
478. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight (1152,)              (1,152 params)
479. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias (4304,)              (4,304 params)
480. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
481. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias (1152,)              (1,152 params)
482. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
483. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias (1152,)              (1,152 params)
484. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
485. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias (1152,)              (1,152 params)
486. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
487. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias (1152,)              (1,152 params)
488. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
489. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias (1152,)              (1,152 params)
490. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
491. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias (1152,)              (1,152 params)
492. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight (1152,)              (1,152 params)
493. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias (1152,)              (1,152 params)
494. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight (1152,)              (1,152 params)
495. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias (4304,)              (4,304 params)
496. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
497. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias (1152,)              (1,152 params)
498. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
499. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias (1152,)              (1,152 params)
500. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
501. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias (1152,)              (1,152 params)
502. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
503. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias (1152,)              (1,152 params)
504. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
505. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias (1152,)              (1,152 params)
506. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
507. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias (1152,)              (1,152 params)
508. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight (1152,)              (1,152 params)
509. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias (1152,)              (1,152 params)
510. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight (1152,)              (1,152 params)
511. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias (4304,)              (4,304 params)
512. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
513. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias (1152,)              (1,152 params)
514. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
515. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias (1152,)              (1,152 params)
516. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
517. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias (1152,)              (1,152 params)
518. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
519. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias (1152,)              (1,152 params)
520. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
521. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias (1152,)              (1,152 params)
522. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
523. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias (1152,)              (1,152 params)
524. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight (1152,)              (1,152 params)
525. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias (1152,)              (1,152 params)
526. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight (1152,)              (1,152 params)
527. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias (4304,)              (4,304 params)
528. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
529. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias (1152,)              (1,152 params)
530. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
531. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias (1152,)              (1,152 params)
532. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
533. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias (1152,)              (1,152 params)
534. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
535. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias (1152,)              (1,152 params)
536. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
537. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias (1152,)              (1,152 params)
538. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
539. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias (1152,)              (1,152 params)
540. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight (1152,)              (1,152 params)
541. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias (1152,)              (1,152 params)
542. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight (1152,)              (1,152 params)
543. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias (4304,)              (4,304 params)
544. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
545. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias (1152,)              (1,152 params)
546. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
547. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias (1152,)              (1,152 params)
548. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
549. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias (1152,)              (1,152 params)
550. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
551. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias (1152,)              (1,152 params)
552. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
553. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias (1152,)              (1,152 params)
554. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
555. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias (1152,)              (1,152 params)
556. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight (1152,)              (1,152 params)
557. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias (1152,)              (1,152 params)
558. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight (1152,)              (1,152 params)
559. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias (4304,)              (4,304 params)
560. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
561. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias (1152,)              (1,152 params)
562. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
563. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias (1152,)              (1,152 params)
564. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
565. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias (1152,)              (1,152 params)
566. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
567. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias (1152,)              (1,152 params)
568. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
569. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias (1152,)              (1,152 params)
570. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
571. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias (1152,)              (1,152 params)
572. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight (1152,)              (1,152 params)
573. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias (1152,)              (1,152 params)
574. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight (1152,)              (1,152 params)
575. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias (4304,)              (4,304 params)
576. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
577. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias (1152,)              (1,152 params)
578. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
579. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias (1152,)              (1,152 params)
580. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
581. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias (1152,)              (1,152 params)
582. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
583. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias (1152,)              (1,152 params)
584. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
585. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias (1152,)              (1,152 params)
586. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
587. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias (1152,)              (1,152 params)
588. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight (1152,)              (1,152 params)
589. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias (1152,)              (1,152 params)
590. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight (1152,)              (1,152 params)
591. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias (4304,)              (4,304 params)
592. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
593. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias (1152,)              (1,152 params)
594. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
595. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias (1152,)              (1,152 params)
596. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
597. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias (1152,)              (1,152 params)
598. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
599. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias (1152,)              (1,152 params)
600. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
601. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias (1152,)              (1,152 params)
602. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
603. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias (1152,)              (1,152 params)
604. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight (1152,)              (1,152 params)
605. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias (1152,)              (1,152 params)
606. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight (1152,)              (1,152 params)
607. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias (4304,)              (4,304 params)
608. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
609. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias (1152,)              (1,152 params)
610. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
611. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias (1152,)              (1,152 params)
612. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
613. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias (1152,)              (1,152 params)
614. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
615. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias (1152,)              (1,152 params)
616. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
617. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias (1152,)              (1,152 params)
618. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
619. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias (1152,)              (1,152 params)
620. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight (1152,)              (1,152 params)
621. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias (1152,)              (1,152 params)
622. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight (1152,)              (1,152 params)
623. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias (4304,)              (4,304 params)
624. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
625. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias (1152,)              (1,152 params)
626. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
627. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias (1152,)              (1,152 params)
628. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
629. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias (1152,)              (1,152 params)
630. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
631. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias (1152,)              (1,152 params)
632. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
633. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias (1152,)              (1,152 params)
634. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
635. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias (1152,)              (1,152 params)
636. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight (1152,)              (1,152 params)
637. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias (1152,)              (1,152 params)
638. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight (1152,)              (1,152 params)
639. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias (4304,)              (4,304 params)
640. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
641. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias (1152,)              (1,152 params)
642. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
643. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias (1152,)              (1,152 params)
644. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
645. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias (1152,)              (1,152 params)
646. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
647. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias (1152,)              (1,152 params)
648. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
649. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias (1152,)              (1,152 params)
650. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
651. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.24.layer_norm1.bias (1152,)              (1,152 params)
652. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.24.layer_norm1.weight (1152,)              (1,152 params)
653. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.24.layer_norm2.bias (1152,)              (1,152 params)
654. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.24.layer_norm2.weight (1152,)              (1,152 params)
655. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.bias (4304,)              (4,304 params)
656. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
657. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.bias (1152,)              (1,152 params)
658. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
659. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.bias (1152,)              (1,152 params)
660. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
661. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.bias (1152,)              (1,152 params)
662. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
663. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.bias (1152,)              (1,152 params)
664. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
665. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.bias (1152,)              (1,152 params)
666. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
667. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.25.layer_norm1.bias (1152,)              (1,152 params)
668. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.25.layer_norm1.weight (1152,)              (1,152 params)
669. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.25.layer_norm2.bias (1152,)              (1,152 params)
670. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.25.layer_norm2.weight (1152,)              (1,152 params)
671. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.bias (4304,)              (4,304 params)
672. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
673. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.bias (1152,)              (1,152 params)
674. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
675. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.bias (1152,)              (1,152 params)
676. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
677. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.bias (1152,)              (1,152 params)
678. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
679. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.bias (1152,)              (1,152 params)
680. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
681. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.bias (1152,)              (1,152 params)
682. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
683. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.26.layer_norm1.bias (1152,)              (1,152 params)
684. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.26.layer_norm1.weight (1152,)              (1,152 params)
685. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.26.layer_norm2.bias (1152,)              (1,152 params)
686. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.26.layer_norm2.weight (1152,)              (1,152 params)
687. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.bias (4304,)              (4,304 params)
688. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
689. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.bias (1152,)              (1,152 params)
690. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
691. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.bias (1152,)              (1,152 params)
692. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
693. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.bias (1152,)              (1,152 params)
694. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
695. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.bias (1152,)              (1,152 params)
696. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
697. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.bias (1152,)              (1,152 params)
698. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
699. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias (1152,)              (1,152 params)
700. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight (1152,)              (1,152 params)
701. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias (1152,)              (1,152 params)
702. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight (1152,)              (1,152 params)
703. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias (4304,)              (4,304 params)
704. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
705. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias (1152,)              (1,152 params)
706. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
707. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias (1152,)              (1,152 params)
708. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
709. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias (1152,)              (1,152 params)
710. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
711. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias (1152,)              (1,152 params)
712. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
713. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias (1152,)              (1,152 params)
714. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
715. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias (1152,)              (1,152 params)
716. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight (1152,)              (1,152 params)
717. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias (1152,)              (1,152 params)
718. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight (1152,)              (1,152 params)
719. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias (4304,)              (4,304 params)
720. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
721. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias (1152,)              (1,152 params)
722. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
723. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias (1152,)              (1,152 params)
724. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
725. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias (1152,)              (1,152 params)
726. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
727. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias (1152,)              (1,152 params)
728. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
729. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias (1152,)              (1,152 params)
730. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
731. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias (1152,)              (1,152 params)
732. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight (1152,)              (1,152 params)
733. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias (1152,)              (1,152 params)
734. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight (1152,)              (1,152 params)
735. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias (4304,)              (4,304 params)
736. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
737. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias (1152,)              (1,152 params)
738. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
739. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias (1152,)              (1,152 params)
740. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
741. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias (1152,)              (1,152 params)
742. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
743. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias (1152,)              (1,152 params)
744. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
745. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias (1152,)              (1,152 params)
746. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
747. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias (1152,)              (1,152 params)
748. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight (1152,)              (1,152 params)
749. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias (1152,)              (1,152 params)
750. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight (1152,)              (1,152 params)
751. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias (4304,)              (4,304 params)
752. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
753. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias (1152,)              (1,152 params)
754. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
755. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias (1152,)              (1,152 params)
756. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
757. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias (1152,)              (1,152 params)
758. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
759. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias (1152,)              (1,152 params)
760. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
761. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias (1152,)              (1,152 params)
762. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
763. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias (1152,)              (1,152 params)
764. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight (1152,)              (1,152 params)
765. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias (1152,)              (1,152 params)
766. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight (1152,)              (1,152 params)
767. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias (4304,)              (4,304 params)
768. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
769. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias (1152,)              (1,152 params)
770. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
771. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias (1152,)              (1,152 params)
772. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
773. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias (1152,)              (1,152 params)
774. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
775. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias (1152,)              (1,152 params)
776. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
777. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias (1152,)              (1,152 params)
778. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
779. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias (1152,)              (1,152 params)
780. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight (1152,)              (1,152 params)
781. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias (1152,)              (1,152 params)
782. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight (1152,)              (1,152 params)
783. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias (4304,)              (4,304 params)
784. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
785. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias (1152,)              (1,152 params)
786. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
787. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias (1152,)              (1,152 params)
788. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
789. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias (1152,)              (1,152 params)
790. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
791. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias (1152,)              (1,152 params)
792. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
793. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias (1152,)              (1,152 params)
794. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
795. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias (1152,)              (1,152 params)
796. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight (1152,)              (1,152 params)
797. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias (1152,)              (1,152 params)
798. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight (1152,)              (1,152 params)
799. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias (4304,)              (4,304 params)
800. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight (4304, 1152)         (4,958,208 params)
801. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias (1152,)              (1,152 params)
802. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight (1152, 4304)         (4,958,208 params)
803. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias (1152,)              (1,152 params)
804. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight (1152, 1152)         (1,327,104 params)
805. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias (1152,)              (1,152 params)
806. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight (1152, 1152)         (1,327,104 params)
807. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias (1152,)              (1,152 params)
808. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight (1152, 1152)         (1,327,104 params)
809. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias (1152,)              (1,152 params)
810. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight (1152, 1152)         (1,327,104 params)
811. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.post_layernorm.bias (1152,)              (1,152 params)
812. model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.post_layernorm.weight (1152,)              (1,152 params)
813. model.time_mlp_in.bias                             (1024,)              (1,024 params)
814. model.time_mlp_in.weight                           (1024, 1024)         (1,048,576 params)
815. model.time_mlp_out.bias                            (1024,)              (1,024 params)
816. model.time_mlp_out.weight                          (1024, 1024)         (1,048,576 params)
817. normalize_actions.buffer_actions.max               (32,)                (32 params)
818. normalize_actions.buffer_actions.min               (32,)                (32 params)
819. normalize_inputs.buffer_state.max                  (32,)                (32 params)
820. normalize_inputs.buffer_state.min                  (32,)                (32 params)
821. normalize_targets.buffer_actions.mean              (32,)                (32 params)
822. normalize_targets.buffer_actions.std               (32,)                (32 params)
823. unnormalize_outputs.buffer_actions.mean            (32,)                (32 params)
824. unnormalize_outputs.buffer_actions.std             (32,)                (32 params)
============================================================
üìä ÊÄªÂèÇÊï∞Èáè: 4,151,795,728 (4151.80M)
